# Jetson Orin Nano éƒ¨ç½²æŒ‡å—

**ç›®æ¨™å¹³å°**: NVIDIA Jetson Orin Nano 8GB
**ä½œæ¥­ç³»çµ±**: JetPack 5.x
**æ—¥æœŸ**: 2026-01-20

---

## ğŸ“‹ ç›®éŒ„

1. [ç¡¬é«”éœ€æ±‚](#ç¡¬é«”éœ€æ±‚)
2. [ç’°å¢ƒè¨­ç½®](#ç’°å¢ƒè¨­ç½®)
3. [æ¨¡å‹å„ªåŒ–](#æ¨¡å‹å„ªåŒ–)
4. [éƒ¨ç½²æµç¨‹](#éƒ¨ç½²æµç¨‹)
5. [æ•ˆèƒ½èª¿å„ª](#æ•ˆèƒ½èª¿å„ª)
6. [ç›£æ§èˆ‡é™¤éŒ¯](#ç›£æ§èˆ‡é™¤éŒ¯)
7. [å¸¸è¦‹å•é¡Œ](#å¸¸è¦‹å•é¡Œ)

---

## ğŸ–¥ï¸ ç¡¬é«”éœ€æ±‚

### æœ€ä½é…ç½®

| é …ç›® | è¦æ ¼ |
|------|------|
| **é–‹ç™¼æ¿** | Jetson Orin Nano 8GB |
| **JetPack** | 5.1.2 æˆ–æ›´é«˜ç‰ˆæœ¬ |
| **å„²å­˜ç©ºé–“** | â‰¥ 64GB (å»ºè­° 128GB NVMe SSD) |
| **é›»æº** | 15W æˆ– 25W æ¨¡å¼ (å»ºè­° 25W) |
| **æ•£ç†±** | ä¸»å‹•æ•£ç†±é¢¨æ‰‡ (å¿…è¦) |
| **ç›¸æ©Ÿ** | ORBBEC Gemini 2 RGB-D Camera |

### å»ºè­°é…ç½®

- **è¨˜æ†¶é«”**: 8GB (æ¨™é…)
- **Swap**: 8GB (éœ€é¡å¤–é…ç½®)
- **æ•£ç†±**: PWM é¢¨æ‰‡ + å°ç†±å¢Š
- **å„²å­˜**: 128GB+ NVMe SSD (M.2 2280)

---

## ğŸ”§ ç’°å¢ƒè¨­ç½®

### 1. å®‰è£ JetPack

1. ä¸‹è¼‰ JetPack SDK Manager:
   ```bash
   # å¾ NVIDIA é–‹ç™¼è€…ç¶²ç«™ä¸‹è¼‰
   # https://developer.nvidia.com/embedded/jetpack
   ```

2. åˆ·å…¥ Jetson Orin Nano:
   ```bash
   # ä¾ç…§å®˜æ–¹æ–‡æª”æ“ä½œ
   # éœ€è¦ Host PC (Ubuntu 18.04/20.04/22.04)
   ```

3. é©—è­‰å®‰è£:
   ```bash
   jetson_release -v
   # ç¢ºèª JetPack ç‰ˆæœ¬ â‰¥ 5.1
   ```

### 2. åŸ·è¡Œè‡ªå‹•è¨­ç½®è…³æœ¬

```bash
cd deployment/jetson
chmod +x setup_jetson.sh
./setup_jetson.sh
```

è…³æœ¬æœƒè‡ªå‹•åŸ·è¡Œ:
- âœ… ç³»çµ±æ›´æ–°
- âœ… å®‰è£ Python ä¾è³´
- âœ… å®‰è£ PyTorch for Jetson
- âœ… é…ç½® swap ç©ºé–“ (8GB)
- âœ… è¨­ç½®æœ€å¤§æ•ˆèƒ½æ¨¡å¼
- âœ… å®‰è£ç›£æ§å·¥å…· (jtop)

### 3. æ‰‹å‹•å®‰è£æ­¥é©Ÿ (é€²éš)

#### å®‰è£ PyTorch

```bash
# PyTorch for Jetson (JetPack 5.x)
wget https://nvidia.box.com/shared/static/[version].whl
pip3 install torch-*.whl

# é©—è­‰
python3 -c "import torch; print(torch.__version__); print(torch.cuda.is_available())"
```

#### å®‰è£ TensorRT

TensorRT é€šå¸¸éš¨ JetPack é è£:

```bash
# é©—è­‰ TensorRT
python3 -c "import tensorrt as trt; print(trt.__version__)"

# å¦‚æœç¼ºå°‘ Python bindings:
cd /usr/src/tensorrt/samples/python/
pip3 install python3-libnvinfer
```

#### é…ç½® Swap

```bash
# å»ºç«‹ 8GB swap
sudo fallocate -l 8G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile

# æ°¸ä¹…å•Ÿç”¨
echo '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab

# é©—è­‰
free -h
```

---

## âš¡ æ¨¡å‹å„ªåŒ–

### å„ªåŒ–æµç¨‹

```mermaid
graph LR
    A[PyTorch æ¨¡å‹] --> B[FP16 é‡åŒ–]
    B --> C[ONNX åŒ¯å‡º]
    C --> D[TensorRT å¼•æ“]
    D --> E[Jetson éƒ¨ç½²]
```

### 1. FP16 é‡åŒ–

```python
from deployment.jetson.model_optimizer import ModelOptimizer

optimizer = ModelOptimizer()

# è½‰æ›ç‚º FP16
model_fp16 = optimizer.convert_to_fp16(
    model,
    save_path="models/model_fp16.pth"
)
```

**æ•ˆæœ**: è¨˜æ†¶é«”æ¸›åŠ, é€Ÿåº¦æå‡ 1.5-2x

### 2. ONNX åŒ¯å‡º

```python
# åŒ¯å‡º ONNX
optimizer.export_to_onnx(
    model,
    input_shape=(1, 3, 224, 224),
    onnx_path="models/model.onnx",
    opset_version=13
)
```

### 3. TensorRT è½‰æ›

```python
# è½‰æ› TensorRT å¼•æ“
optimizer.convert_to_tensorrt(
    onnx_path="models/model.onnx",
    engine_path="models/model_fp16.engine",
    precision="fp16",
    workspace_size=2,  # GB
    max_batch_size=1
)
```

**æ•ˆæœ**: é€Ÿåº¦æå‡ 2-3x (ç›¸æ¯” FP16)

### å„ªåŒ–å°æ¯”

| ç‰ˆæœ¬ | è¨˜æ†¶é«” | é€Ÿåº¦ | ç²¾åº¦ | å»ºè­° |
|------|--------|------|------|------|
| **FP32** | 100% | åŸºæº– | æœ€é«˜ | é–‹ç™¼æ¸¬è©¦ |
| **FP16** | 50% | 1.5-2x | æ¥è¿‘ | â­ï¸ å¹³è¡¡æ¨¡å¼ |
| **INT8** | 25% | 3-4x | ç•¥é™ | æ¥µè‡´æ•ˆèƒ½ |
| **TensorRT FP16** | 50% | 2-3x | æ¥è¿‘ | â­ï¸ ç”Ÿç”¢éƒ¨ç½² |

---

## ğŸš€ éƒ¨ç½²æµç¨‹

### 1. æº–å‚™æ¨¡å‹æª”æ¡ˆ

```bash
# ç›®éŒ„çµæ§‹
deployment/
â”œâ”€â”€ jetson/
â”‚   â”œâ”€â”€ setup_jetson.sh
â”‚   â”œâ”€â”€ model_optimizer.py
â”‚   â””â”€â”€ resource_monitor.py
â””â”€â”€ models/
    â”œâ”€â”€ yolo11n_fp16.engine       # ç‰©ä»¶æª¢æ¸¬
    â”œâ”€â”€ segmentation_fp16.engine  # åˆ†å‰²
    â””â”€â”€ config.json               # é…ç½®
```

### 2. é…ç½®ç³»çµ±

```python
# config.json
{
  "camera": {
    "width": 1280,
    "height": 800,
    "fps": 30
  },
  "models": {
    "detection": "models/yolo11n_fp16.engine",
    "segmentation": "models/segmentation_fp16.engine"
  },
  "performance": {
    "mode": "balanced",  # high_quality, balanced, high_speed
    "subsample": 4,
    "enable_temporal_filter": true
  }
}
```

### 3. åŸ·è¡Œç³»çµ±

```bash
# è¨­ç½®æ•ˆèƒ½æ¨¡å¼
sudo nvpmodel -m 0  # æœ€å¤§æ•ˆèƒ½
sudo jetson_clocks   # é–å®šæ™‚è„ˆ

# åŸ·è¡Œæª¢æ¸¬ç³»çµ±
python3 scripts/demo_e2e.py --config deployment/config.json
```

### 4. æ•ˆèƒ½ç›£æ§

```bash
# çµ‚ç«¯ 1: åŸ·è¡Œç³»çµ±
python3 scripts/demo_e2e.py

# çµ‚ç«¯ 2: ç›£æ§è³‡æº
python3 deployment/jetson/resource_monitor.py

# æˆ–ä½¿ç”¨ jtop
sudo jtop
```

---

## ğŸ¯ æ•ˆèƒ½èª¿å„ª

### æ•ˆèƒ½æ¨¡å¼è¨­å®š

Jetson Orin Nano æœ‰ 2 ç¨®æ•ˆèƒ½æ¨¡å¼:

| æ¨¡å¼ | åŠŸç‡ | æ•ˆèƒ½ | å»ºè­°ä½¿ç”¨ |
|------|------|------|----------|
| **Mode 0** (MAXN) | 25W | æœ€é«˜ | â­ï¸ ç”Ÿç”¢ç’°å¢ƒ |
| **Mode 1** (15W) | 15W | ä¸­ç­‰ | é–‹ç™¼æ¸¬è©¦ |

```bash
# æŸ¥çœ‹ç•¶å‰æ¨¡å¼
sudo nvpmodel -q

# è¨­ç½®æœ€å¤§æ•ˆèƒ½æ¨¡å¼
sudo nvpmodel -m 0

# é–å®šæ™‚è„ˆ (é¿å…é™é »)
sudo jetson_clocks

# è¨­ç½®é¢¨æ‰‡ç‚ºæœ€å¤§è½‰é€Ÿ
sudo jetson_clocks --fan
```

### ç³»çµ±å„ªåŒ–å»ºè­°

#### 1. è¨˜æ†¶é«”å„ªåŒ–

```python
# ä½¿ç”¨ FP16 æ¨¡å‹
model = model.half()

# æ¸›å°‘æ‰¹æ¬¡å¤§å°
batch_size = 1

# ä½¿ç”¨æ¢¯åº¦æª¢æŸ¥é» (å¦‚æœè¨“ç·´)
torch.utils.checkpoint.checkpoint(...)
```

#### 2. å½±åƒè™•ç†å„ªåŒ–

```python
from src.processing import PerformanceOptimizer

optimizer = PerformanceOptimizer()

# å¿«é€Ÿæ¿¾æ³¢
depth_filtered = optimizer.fast_bilateral_filter(depth, d=5)

# æ™‚åŸŸæ¿¾æ³¢
depth_filtered = optimizer.temporal_filter(depth_filtered, alpha=0.7)

# è‡ªé©æ‡‰é™æ¡æ¨£
subsample = optimizer.adaptive_subsample(depth, target_points=30000)
points = optimizer.fast_pointcloud_generation(
    depth, fx, fy, cx, cy, subsample=subsample
)
```

#### 3. å¤šåŸ·è¡Œç·’å„ªåŒ–

```python
import threading
from queue import Queue

# ç›¸æ©ŸåŸ·è¡Œç·’
def camera_thread(queue):
    while True:
        frame = camera.get_frame()
        queue.put(frame)

# è™•ç†åŸ·è¡Œç·’
def processing_thread(queue):
    while True:
        frame = queue.get()
        result = process(frame)
```

### é æœŸæ•ˆèƒ½

#### æœ€ä½³åŒ–é…ç½® (FP16 + TensorRT)

| æ¨¡çµ„ | è€—æ™‚ | FPS |
|------|------|-----|
| **ç›¸æ©Ÿæ“·å–** | ~30 ms | 33 |
| **AI æ¨ç†** (YOLOv11n FP16) | ~15 ms | 67 |
| **é»é›²ç”Ÿæˆ** (subsample 4x) | ~5 ms | 200 |
| **é‡æ¸¬ + æ±ºç­–** | ~10 ms | 100 |
| **ç¸½è¨ˆ** | ~60 ms | **15-20 FPS** |

#### å¹³è¡¡é…ç½® (FP16 ç„¡ TensorRT)

| æ¨¡çµ„ | è€—æ™‚ | FPS |
|------|------|-----|
| **ç›¸æ©Ÿæ“·å–** | ~30 ms | 33 |
| **AI æ¨ç†** (FP16) | ~40 ms | 25 |
| **é»é›²ç”Ÿæˆ** | ~5 ms | 200 |
| **é‡æ¸¬ + æ±ºç­–** | ~10 ms | 100 |
| **ç¸½è¨ˆ** | ~85 ms | **10-12 FPS** |

---

## ğŸ“Š ç›£æ§èˆ‡é™¤éŒ¯

### ä½¿ç”¨è³‡æºç›£æ§å™¨

```bash
# åŸ·è¡Œç›£æ§å™¨
python3 deployment/jetson/resource_monitor.py

# é€£çºŒç›£æ§ 60 ç§’
python3 -c "
from deployment.jetson.resource_monitor import ResourceMonitor
monitor = ResourceMonitor()
monitor.monitor_continuous(duration=60, interval=1.0)
"
```

### ä½¿ç”¨ jtop

```bash
# å®‰è£
sudo pip3 install jetson-stats

# åŸ·è¡Œ
sudo jtop

# å¿«æ·éµ:
# 1: ç³»çµ±è³‡è¨Š
# 2: CPU/GPU ä½¿ç”¨ç‡
# 3: è¨˜æ†¶é«”
# 4: æº«åº¦
# 5: åŠŸç‡
```

### æ•ˆèƒ½åˆ†æ

```python
from src.processing import Timer

with Timer() as t:
    result = model(input_data)
print(f"æ¨ç†æ™‚é–“: {t.elapsed*1000:.2f} ms")

# è©³ç´°åˆ†æ
import torch.profiler as profiler

with profiler.profile() as prof:
    model(input_data)

prof.export_chrome_trace("trace.json")
# åœ¨ chrome://tracing æŸ¥çœ‹
```

### å¸¸è¦‹æ•ˆèƒ½å•é¡Œ

#### 1. CPU/GPU ä½¿ç”¨ç‡ä½

```bash
# æª¢æŸ¥æ˜¯å¦å•Ÿç”¨æœ€å¤§æ•ˆèƒ½æ¨¡å¼
sudo nvpmodel -q

# é–å®šæ™‚è„ˆ
sudo jetson_clocks
```

#### 2. è¨˜æ†¶é«”ä¸è¶³

```bash
# æª¢æŸ¥ swap
free -h

# å¢åŠ  swap
sudo swapoff /swapfile
sudo fallocate -l 16G /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
```

#### 3. éç†±é™é »

```bash
# æª¢æŸ¥æº«åº¦
sudo jtop  # æŸ¥çœ‹æº«åº¦

# ç¢ºä¿æ•£ç†±
# - ç¢ºèªé¢¨æ‰‡é‹ä½œ
# - æ¸…ç†ç°å¡µ
# - æ›´æ›å°ç†±å¢Š
```

---

## â“ å¸¸è¦‹å•é¡Œ

### Q1: PyTorch å®‰è£å¤±æ•—?

**A**: ç¢ºä¿ä½¿ç”¨ Jetson å°ˆç”¨çš„ PyTorch wheel:

```bash
# å¾ NVIDIA è«–å£‡ä¸‹è¼‰å°æ‡‰ JetPack ç‰ˆæœ¬çš„ wheel
# https://forums.developer.nvidia.com/t/pytorch-for-jetson/72048

# ä¸è¦ä½¿ç”¨ pip install torch
pip3 install torch-*.whl
```

### Q2: TensorRT å¼•æ“å»ºç«‹å¤±æ•—?

**A**: æª¢æŸ¥:
1. ONNX æ¨¡å‹æ˜¯å¦æ­£ç¢ºåŒ¯å‡º
2. workspace_size æ˜¯å¦è¶³å¤ 
3. è¨˜æ†¶é«”æ˜¯å¦å……è¶³ (è€ƒæ…®å¢åŠ  swap)

```bash
# å¢åŠ  swap
sudo fallocate -l 16G /swapfile
```

### Q3: ç›¸æ©Ÿç„¡æ³•é€£æ¥?

**A**: ç¢ºèª:
```bash
# æª¢æŸ¥ USB è£ç½®
lsusb | grep Orbbec

# æª¢æŸ¥æ¬Šé™
sudo chmod 666 /dev/bus/usb/*/*

# å®‰è£é©…å‹•
# åƒè€ƒ ORBBEC SDK æ–‡æª”
```

### Q4: æ•ˆèƒ½ä¸å¦‚é æœŸ?

**A**: æª¢æŸ¥åˆ—è¡¨:
- [ ] æ˜¯å¦ä½¿ç”¨ FP16/TensorRT
- [ ] æ•ˆèƒ½æ¨¡å¼æ˜¯å¦è¨­ç‚º MAXN
- [ ] æ˜¯å¦æœ‰éç†±é™é »
- [ ] è¨˜æ†¶é«”æ˜¯å¦å……è¶³
- [ ] æ˜¯å¦ä½¿ç”¨è‡ªé©æ‡‰é™æ¡æ¨£

### Q5: å¦‚ä½•é€²ä¸€æ­¥æå‡æ•ˆèƒ½?

**A**: å„ªåŒ–ç­–ç•¥:
1. **INT8 é‡åŒ–** (éœ€è¦æ ¡æº–è³‡æ–™)
2. **æ¨¡å‹å‰ªæ** (æ¸›å°‘åƒæ•¸é‡)
3. **çŸ¥è­˜è’¸é¤¾** (ä½¿ç”¨æ›´å°çš„æ¨¡å‹)
4. **å¤šåŸ·è¡Œç·’** (ç›¸æ©Ÿèˆ‡æ¨ç†ä¸¦è¡Œ)
5. **æ‰¹æ¬¡è™•ç†** (ç´¯ç©å¤šå¹€ä¸€èµ·è™•ç†)

---

## ğŸ“ éƒ¨ç½²æª¢æŸ¥æ¸…å–®

- [ ] JetPack 5.x å·²å®‰è£
- [ ] Python ç’°å¢ƒå·²é…ç½®
- [ ] PyTorch for Jetson å·²å®‰è£
- [ ] TensorRT å¯ç”¨
- [ ] Swap ç©ºé–“ â‰¥ 8GB
- [ ] æ•ˆèƒ½æ¨¡å¼è¨­ç‚º MAXN
- [ ] é¢¨æ‰‡æ­£å¸¸é‹ä½œ
- [ ] æ¨¡å‹å·²è½‰æ›ç‚º FP16/TensorRT
- [ ] ç›¸æ©Ÿé©…å‹•å·²å®‰è£
- [ ] æ¸¬è©¦ç¨‹å¼æ­£å¸¸é‹ä½œ
- [ ] è³‡æºç›£æ§å·¥å…·å¯ç”¨
- [ ] æ•ˆèƒ½ç¬¦åˆé æœŸ (â‰¥15 FPS)

---

## ğŸ“š åƒè€ƒè³‡æ–™

- [NVIDIA Jetson Orin Nano æ–‡æª”](https://developer.nvidia.com/embedded/jetson-orin-nano-developer-kit)
- [JetPack SDK](https://developer.nvidia.com/embedded/jetpack)
- [PyTorch for Jetson](https://forums.developer.nvidia.com/t/pytorch-for-jetson/72048)
- [TensorRT æ–‡æª”](https://docs.nvidia.com/deeplearning/tensorrt/)
- [jetson-stats](https://github.com/rbonghi/jetson_stats)

---

**æ–‡æª”ç‰ˆæœ¬**: 1.0
**æœ€å¾Œæ›´æ–°**: 2026-01-20
**ä½œè€…**: Claude Sonnet 4.5 + Happy
